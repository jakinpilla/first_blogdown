---
title: "로지스틱 회귀(Logistic Regression)의 기본 원리"
author: "Daniel Kim"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
    html_document:
      toc : true
      toc_depth : 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", message=F, warning=F, fig.width = 6, fig.height = 4)
```


```{r message = FALSE, warning = FALSE, echo=FALSE}
library(ez)
library(ggplot2)
library(nlme)
library(pastecs)
library(reshape2)
# library(WRS)
library(clinfun)
library(pgirmess)
library(car)
library(tidyverse)
# install.packages('mvoutlier')
library(mvoutlier)
library(gmodels)
library(MASS)
library(nlme) # 다층모형을 위해
library(QuantPsyc)
library(boot)
# install.packages('epiDisplay')
library(epiDisplay)
library(knitr)
# install.packages('mlogit')
library(mlogit)
library(plotly)
```

##  로지스틱 회귀란?

 \ 로지스틱 회귀는 결과변수가 범주형 변수이고 예측변수들이 연속변수 또는 범주형 변수인 회귀분석 모형입니다. 보통 머신러닝에서 분류(**Classification**)을 위해 가장 기본이 되는 모형으로 사용됩니다. 이 때 결과변수가 두 가지일(0 & 1, False & True, Neg & Pos 등) 경우 **Binary Logistic Regression**라 하고, 결과변수가 셋 이상일 경우 **Multinomial Logistic Regression** 이라고 합니다.


## 로지스틱 회귀의 원리

 \ 예측변수가 $X_{1i}$ 하나 뿐인 가장 단순한 경우에, Y가 1이 될(혹은  True, Pos 등, 상대적으로 관심이 있어 예측을 하고 싶은 대상을 1로 둡니다.)  **확률** $P(Y = 1 | X_{1i})$ 즉 **관측값이 $X_{1i}$일 때 예측값이 1이 될 확률**을 구하기 위한 로지스틱 회귀 방정식은 다음과 같습니다.

$$P(Y=1 | X_{1i}) = \frac{1}{1 + e^{-(b_{0} + b_{1}X_{1i})}}$$

이 식을 변수 $X_{1i}$에 대한 그래프로 그리면 다음과 같습니다.

```{r, fig.align='center'}
myfun <- function(x_var) {1 / (1 + exp(-x_var))};
ggplot(data.frame(x= c(-10, 10)), aes(x=x)) + 
  stat_function(fun = myfun, geom="line", size = 1.2, color = 'steelblue') + 
  xlab("Z") +
  ylab("P(Y)") +
  theme_minimal() -> p1; ggplotly(p1)
```

 \ 여기서 주목할 점은 $X$가 $- \infty$에서 $\infty$ 까지 변한다 해도 $P(Y=1 | X_{1i})$의 값은 0에서 1까지 변한다는 점입니다. 앞에서 잠깐 언급한 대로 $P(Y=1 | X_{1i})$는 확률이고 당연히 그 범위는 0에서 1이어야 합니다.

 \ 위와 같은 $S$ 모양과 비슷한 곡선을 로지스틱 곡선이라고 합니다. 만약 데이터가 $X$가 작을 때 결과변수가 0인 데이터가 다수 관측되어 지다가 $X$가 증가하면서 결과변수 값이 1인 데이터가 다수가 되는 상황이라면, 이러한 비율을 가지는 데이터를 이 로지스틱 곡선에 **끼워 맞추는** 것이 로지스틱 회귀분석이라고 할 수 있습니다. 
 
 \ 여기서 다시 처음으로 돌아가 생각해보겠습니다. 어떻게 로지스틱 회귀 방정식은 다음과 같이 만들어진 걸까요?
 
 
 $$ P(Y=1 | X_{1i}) = \frac{1}{1 + e^{-(b_{0} + b_{1}X_{1i})}}$$

 \ 제 경우 이것을 이해하기란 쉽지 않았습니다. 하지만 지금부터 제가 이해한 만큼 설명해보려 합니다. 
 
 \ 먼저 우리가 알고 있는 선형회귀는 선형성이라는 가정을 가지고 있습니다. 즉, 선형회귀의 모형이 타당하려면, 해당 관측자료의 결과변수와 예측변수간에 관계가 선형이어야 합니다. 그런데 결과변수가 0과 1일 경우 이러한 선형성 가정이 더 이상 성립하지 않게 됩니다. 여기서 바로 이런 선형성 가정을 성립시키기 위해 특별히 고안해낸 방법을 도입하게 됩니다. **바로, 확률인 결과변수에 로짓변환을 시켜 선형인 예측변수와 연결하는 방법이 그것입니다.**
 
 \ 먼저 로짓변환부터 알아보겠습니다. 0과 1사이에서 변화하는 확률인 $P(Y=1|X_{1i})$은 설명변수인 $X_{1i}$의 함수입니다. 이러한 함수를 $\mu(X_{1i})$라고 하겠습니다. 이러한 $\mu(X_{1i})$가 0에서 1까지 변화하면
 
 $$\frac{\mu(X_{1i})}{1-\mu(X_{1i})}$$
 
 \ 위의 식은 다음 그래프에서 보는 바와 같이 0에서 $\infty$ 까지 변화하게 됩니다.

```{r}
myfun_1 <- function(mu_var) {mu_var / (1 - mu_var)};
ggplot(data.frame(x= c(0, 1)), aes(x=x)) + 
  stat_function(fun = myfun_1, geom="line", size = 1.2, color = 'steelblue') + 
  xlab("mu") +
  ylab(paste("odds")) +
  theme_minimal() -> p_2; ggplotly(p_2)
```


\ 참고로 이 식에 대해 좀 더 생각해보겠습니다. 이 식의 분자 $\mu(X_{1i})$는 관측값이 $X_{1i}$일 때 예측값이 1이 될 확률   $P(Y = 1 | X_{1i})$입니다. 반대로 분모 $1-\mu(X_{1i})$는 해당 확률을 1에서 뺀 값이므로 관측값이 $X_{1i}$일 때 예측값이 0이 될 확률이라고 할 수 있습니다. 즉 개념적으로 이 식을 설명하면 다음과 같이 표현할 수 있습니다.

$$\frac{P(Y = 1 | X_{1i})}{P(Y = 0 | X_{1i})}$$

\  $P(Y = 1 | X_{1i})$은 예측값이 1이 되는 확률로 결과변수가 1이 되는 사건이 발생할 확률로 생각할 수 있고, 반대로  $P(Y = 0 | X_{1i})$은 예측값이 0이 되는 사건이 발생하는 확률로 생각할 수 있습니다. 이러한 수식을 일컬어서 **오즈(odds)**라고 합니다.

\ 바로 이 0부터  $\infty$ 까지 변화하는 오즈(odds)에 로그를 씌우면 전체의 값이 $-\infty$에서 $+\infty$까지 변화하게 됩니다. 즉 다음의 식은 변화범위가 $-\infty$에서 $+\infty$까지가 됩니다.

 $$log \left(\frac{\mu(X_{1i})}{1-\mu(X_{1i})} \right)$$

```{r}
myfun_2 <- function(mu_var) {log(mu_var / (1 - mu_var))};
ggplot(data.frame(x= c(0, 1)), aes(x=x)) + 
  stat_function(fun = myfun_2, geom="line", size = 1.2, color = 'steelblue') + 
  labs(x = "mu", y = "Y") +
  theme_minimal() -> p_3; ggplotly(p_3)
```


 바로 이러한 변환을 로짓변환이라고 합니다. 
 
 \ 확률값인 $\mu(X_{1i})$를 로짓변환을 통해 $-\infty$에서 $+\infty$까지 변화되는 값으로 변환시키고 이렇게 변환된 값을 마찬가지로 $-\infty$에서 $+\infty$까지 변화할 수 있는 선형예측 함수인 $\beta_{1} X_{1i}$와 연결하여 선형성 가정 위반을 회피합니다. 이를 수식으로 표현하면 다음과 같게 됩니다.
 
$$log \left(\frac{\mu(X_{1i})}{1-\mu(X_{1i})} \right) = b_{0} + b_{1}X_{1i}$$ 
 
 \ 위의 수식을 고등학교 수학 실력을 총동원해 $\mu(X_{1i})$에 관한 식으로 정리하면 다음과 같게 됩니다.
 
$$\mu(X_{1i}) = \frac{e^{(b_{0} + b_{1}X_{1i})}}{1+e^{(b_{0} + b_{1}X_{1i})}}$$

\ 여기서 한 번 더 고등학교 수학실력을 발휘하면

$$P(Y=1|X_{1i}) = \mu(X_{1i}) = \frac{1}{1+e^{-(b_{0} + b_{1}X_{1i})}}$$

 \ 즉 처음에 등장한 로지스틱 회귀방정식이 나오게 되는 것입니다.

 \ 예측변수가 여러 개($X_{1i}, X_{2i}, ..., X_{ni}$)인 로지스틱 회귀 방정식은 단순히 변수들을 위와 동일한 식에 추가하여 주기만 하면 되고 결국 다음과 같게 됩니다.

$$P(Y) = \frac{1}{1 + e^{-(b_{0} + b_{1}X_{1i} + b_{2}X_{2i} + ... + b_{n}X_{ni})}}$$

- (요약정리) 선형회귀의 모형이 타당하려면, 해당 관측 자료에 선형 관계가 존재해야 한다.  그런데 결과변수가 범주형이면 그러한 가정이 깨진다. 로짓변환은 비선형 관계를 선형 관계로 표현하는 한 방법이다. 로지스틱 회귀 방정식은 바로 그러한 원리에 기초한다. 즉, 다중 선형 관계를 로그 항(logit)들로 표현함으로써 선형성 가정 위반 문제를 극복한다.

