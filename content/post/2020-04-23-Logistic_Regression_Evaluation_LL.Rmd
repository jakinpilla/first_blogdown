---
title: "로지스틱 회귀(Logistic Regression) 모형의 평가: 로그우도, 이탈도 등"
author: "Daniel Kim"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
    html_document:
      toc : true
      toc_depth : 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", message=F, warning=F, cache=T)
```


```{r message = FALSE, warning = FALSE, echo=FALSE}
library(ez)
library(ggplot2)
library(nlme)
library(pastecs)
library(reshape2)
library(WRS)
library(clinfun)
library(pgirmess)
library(car)
library(tidyverse)
# install.packages('mvoutlier')
library(mvoutlier)
library(gmodels)
library(MASS)
library(nlme) # 다층모형을 위해
library(QuantPsyc)
library(boot)
# install.packages('epiDisplay')
library(epiDisplay)
library(knitr)
# install.packages('mlogit')
library(mlogit)
library(plotly)
```


## 로지스틱 회귀 모형의 평가: 로그 우도 통계량

 \ 로지스틱 회귀에서도 선형회귀에서와 마찬가지로  관측값과 예측값을 비교해서 모형의 적합도를 평가합니다. 이때 쓰이는 척도가 로그우도(log-likelihood)입니다. 로지스틱 회귀 모델에서 로그우도를 계산하는 식은 다음과 같습니다.

$$LL = \sum_{i=1}^{N} \left[  Y_{i} \ln(P(Y_{i})) + (1-Y_{i})\ln(1 - P(Y_{i}))  \right]$$

 \ 머신러닝 학습을 하게 되면 **우도**라는 말이 자주 등장합니다. 그리고 그와 함께 따라 다니는 개념이 **최대우도추정(Maximum Likelihood Estimation)**입니다. 우도와 최대우도추정을 이번 포스트에서 모두 다루기는 어렵습니다. 언젠가 자세한 설명을 한 포스트를 올리겠습니다. 
 
 \ 다만, 이번 포스트에서는 왜 로그가능도를 계산하는 식이 위와같이 나왔는가에 대한 설명을 수학적으로만 하려고 합니다. 다소 어려운 개념을 포함하므로 이해하기 어려우시면 다음 올리는 포스트를 참고하여 주시기 바랍니다.

### 로지스틱 회귀 결과변수의 확률분포
 
 \ 로지스틱 회귀의 결과변수는 0 또는 1의 값을 가지는 베르누이 확률변수입니다.(뭐지 이 베르누이는?...) 이 말을 $Y \text{~} Brenoulli(P)$라고도 표현합니다. 만약 1이 나올 확률이 $P$라고 한다면 반대로 0이 나올 확률은 $1-P$입니다. 
 
  \ 여기서의 $P$를 좀 더 쉽게 이해해 볼까요? 저는 하얀 바둑알과 검은 바둑알이 마구 섞여져 있는 주머니를 머리속에서 상상하곤 합니다.  이 바둑알들의 크기와 모양은 완전히 동일합니다 .저는 지금 이 주머니에 얼마나 많은 바둑알 들이 서로 몇 개씩 섞여 있는지 상태를 모릅니다. 이런 상황속에서 제가 주머니에 손을 넣어 바둑알 1개를 꺼내었더니 하얀 바둑알이 나왔고, 그 꺼내진 바둑알을 다시 주머니에 넣고 마구 흔들어 다시 바둑알을 꺼내었더니 이번에는 검은 바둑알이 나왔다고 하겠습니다. 이런 장난(실험...)을 하면서 저는 이 주머니 속에서 검은 바둑알을 꺼낼 확률이 매우 알고 싶어졌습니다. 그래서 검은 바둑알이 나올 확률을 그 값을 모르지만 일단 $P$라고 두었습니다. 그럼 반대로 하얀 바둑알이 나올 확률은 전체 확률에서 검은 바둑알 꺼낼 확률을 제외한 값이므로 $1-P$가 됩니다. 
  
  \ 그런데 저는 위에서 관심이 있는 사건이 검은 바둑알을 꺼내는 사건이고 이 검은 바둑알을 꺼내는 사건에 대한 확률을 $P$라고 했습니다. 즉 결과인 $Y_{i}$를 검은 바둑알이면 1, 하얀 바둑알이면 0으로 생각한다는 의미입니다. 
 
  \ 위와 같은 상황을 좀 더 수식으로 표현하면 다음과 같습니다. 
 
$$ \text{Bernoulli}(Y;P) = \begin{cases} P & \text{if black } Y_{i} = 1 \\ 1 - P & \text{if white } Y_{i} = 0 \end{cases}$$

 \ 자...여기서 만약 위의 표현식을 조건문 없이 표현한다고 하면 어떻게 될까요? 
 
 $$ \text{Bernoulli}(Y;P) =  P^{Y_{i}}(1-P)^{1-Y_{i}} \quad \quad (Y_{i} = 1, 0)$$
 
 \ $Y_{i}$에 직접 0과 1을 대입하여 생각해보면 위의 식과 아래의 식이 동일함을 알 수 있습니다. $Y_{i} = 1$이면 전체적인 식이 단순히 $P$이 되고 $Y_{i} = 0$이면 전체적인 식이 단순히 $1-P$이 가 되기 때문입니다.
 
 \ 결론적으로 결과변수는 $P^{Y_{i}}(1-P)^{1-Y_{i}}$인 확률분포를 따릅니다.

### 로그를 씌우는 이유

 \ 자 이제부터 위의 식을 가지고 좀 더 나아가 생각해보겠습니다. 위의 식에서 $i$ 첨자는 $i$번째를 의미합니다. 정확히는 $i$번째 사건을 의미하지요. 그렇다면 현재 두 번의 사건이 일어났는데 이 두 번의 사건이 일어날 수있는 확률은 어떻게 계산할까요?
 
 \ 우리는 여기서 두 사건이 독립이라는 가정을 하게 됩니다. 즉 앞의 사건이 뒤의 사건에 영향을 미치지 않는다는 의미입니다. 앞서 저는 바둑알을 꺼낸 후 다시 주머니에 넣는다고 했습니다. 즉 앞에서 제가 어떤 바둑알을 꺼냈든지 뒤의 사건에는 영향이 없습니다. 이런 경우 우리는 사건들이 서로 독립이다라고 표현합니다.  그런데 사건이 독립일 때 이 두 사건이 일어날 확률은 단순히 각각의 사건이 일어날 확률을 곱하는 것입니다. 즉,
 
 $$P^{Y_{1}}(1-P)^{1-Y_{1}} \times P^{Y_{2}}(1-P)^{1-Y_{2}}$$

\ 위와 같은 수식을 좀 더 간결하고 우아하게 표현하면 파이 프로덕트(product) 연산자를 이용하여 다음과 같이 표현할 수 있습니다.

 $$\prod_{i=1}^{2} P^{Y_{i}}(1-P)^{1-Y_{i}}$$

\ 이것을 두 번의 사건이 아닌 N번의 사건이 발생한 경우의 확률을 구하는 식으로 변환하려면 단순히 2를 N으로 바꾸어 주면 됩니다. 그래서 결국 첫 번째 사건부터 N번째 사건이 발생할 확률을 구하면 다음의 식이 나오게 됩니다.

$$\prod_{i=1}^{N} P^{Y_{i}}(1-P)^{1-Y_{i}}$$
 
 \ 이것이 바로 로지스틱 회귀의 결과변수에 대한 우도(likelihood)에 대한 수식입니다. 예를 들어보면 제가 처음에 하얀 색, 두 번째는 검은 색, 세번째는 하얀 색의 바둑알을 주머니에서 뽑아 총 세 번의 사건이 발생했다라고 한다면 이 사건의 우도는 $(1-P) \times P \times (1-P)$입니다.
 
 \ 그런데 이 식은 곱하기로 연결되어 있어서 미분 계산을 하기 어렵습니다. 추후 우리는 최대우도추정을 위해 미분을 통해 우도가 최대가 되는 지점을 찾아내야 하는 상황과 마주쳐야 하는데 미분을 하기 너무 어려운 이 식은 부담이 됩니다. 그래서 여기서 하나의 트릭이 등장하하는데 바로 위의 식에 로그를 씌우는 것입니다. 
 
 \ 로그는 로그 안의 수식이 곱으로 연결되어 있을 때 이를 더하기로 분리해 줍니다.(이는 고등학교때 배우셨...;;;) 파이 product 연산자는 시그마 sum 연산자로 변화되고 안의 항들의 지수들은 앞으로 튀어나와 단순히 하나의 항으로 뒤의 항들과 곱해집니다. 이런 수식은 앞의 수식보다 휠씬 미분 계산을 하기가 쉽습니다.즉 위의 식에 로그를 씌우면 다음과 같이 변신합니다. 바로 처음에 등장했던 로지스틱 회귀 모형의 로그우도에 관한 식입니다. 
 
 $$\sum_{i=1}^{N} \left[  Y_{i} \ln(P) + (1-Y_{i})\ln(1 - P)  \right]$$
 
 \ 위 식의 의미를 좀 더 자세히 살펴보겠습니다. 만약 $Y_{i} = 1$이면 위의 식은 
 
 $$ \sum_{i=1}^{N} ln(P) $$
 
이 됩니다.  만약  $Y_{i} = 1$ 일 때  $Y_{i} = 1$이 되는 확률을 정확히 $P=1$로 예측했다면 우도는 0이 됩니다. 반대로 $P=0$로 예측했다면 우도는 $-\infty$가 됩니다. 

반대의 경우를 생각해보겠습니다. 만약  $Y_{i} = 0$ 일 때  우도의 수식은 다음과 같아집니다.

 $$ \sum_{i=1}^{N} ln(1-P) $$

이 경우, $Y_{i} = 1$이 되는 확률을 정확히 $P=0$로 예측했다면 우도는 0이 됩니다. 반대로 $P=0$로 예측했다면 우도는 $-\infty$가 됩니다.

 
 \ 우리는 보통 우도의 크기가 상대적으로 큰 지 작은 지에 대해서만 관심이 있습니다.  로그함수는 단조증가함수로 변수의 크기가 크면 로그함수의 값도 크고 작으면 로그함수의 값도 작습니다. 아래 그림에서 금방 확인하실 수 있습니다.

```{r}
myfun_1 <- function(x_var) {log(x_var)};
ggplot(data.frame(x= c(0, 10)), aes(x=x)) + 
  stat_function(fun = myfun_1, geom="line", size = 1.2, color = 'steelblue') + 
  labs(x = "X", y = "Y") +
  theme_minimal() -> p_1; ggplotly(p_1)
```


\ 즉, 로그함수의 값이 작으면 원래의 함수값도 작은 것이고 로그함수 값이 크면 원래의 함수값도 큰 것입니다.(비록 변화율은 차이가 나지만...) 즉, 로그함수의 값이 최대가 되는 지점이 전체 함수가 최대가 되는 지점이 됩니다. 이런 성질을 활용해 최대우도추정에 활용합니다. 

### 로그우도 통계량의 특징

\ 앞서 알아본 로그우도 통계량은 다음곽 같은 특징들이 있습니다. 

- 로그우도 통계량은는 예측값들과 실제 관측값들에 관한 확률들의 합입니다.

- 로그우도 통계량은 모형이 적합된 후에도 여전히 설명되지 않는 정보의 양을 나타냅니다. 이는 마치 다중회귀의 잔차제곱합($SSR$)과 비슷합니다. 로그가능도 통계량이 크다는 것은 설명되지 않은 관측이 많이 남아 있다는 뜻입니다. 

- 로그가능도 통계량이 큰 통계적 모형은 자료에 별로 적합하지 않다고 할 수 있습니다.


### 모형의 평가: 이탈도 통계량

\ 로지스틱 회귀에서 이탈도는 로그우도와 아주 밀접하게 관련이 있습니다. 이탈도를 정의하는 수식은 다음과 같습니다. 

$$Logistic \ Regression \ Deviance = -2 \times LL = \sum_{i=1}^{N} \left[ -2 Y_{i} \ln(P) -2(1-Y_{i})\ln(1 - P)  \right]$$

 \ 로지스틱 모형을 평가하는데 있어 앞서 살펴본  로그우도를 그대로 사용하는 것보다는 이 이탈도를 사용합니다. 왜냐하면 이탈도는 카이제곱 분포를 따르기 때문입니다.
 
 \ 여기서 잠깐 위의 식에 대해 생각해 보겠습니다.  $Logistic \ Regression \ Deviance$는 $Y_{i} = 1$ 일때 다음의 수식이 됩니다. 
 
$$\sum_{i=1}^{N} \left[ -2 \ln(P) \right]$$

 \ 여기서 P는 0부터 1까지 변화하므로 위의 식의 값은 $+\infty$에서 0까지 변화합니다.
 
```{r}
myfun_2 <- function(x_var) {-2*log(x_var)};
ggplot(data.frame(x= c(0, 1)), aes(x=x)) + 
  stat_function(fun = myfun_2, geom="line", size = 1.2, color = 'steelblue') + 
  labs(x = "P", y = "Logistic Reg Deviance") +
  theme_minimal() -> p_2; ggplotly(p_2)
```

 \ $Y_{i} = 1$일때 만약 결과변수가 1이 되는 확률 $P =1$ 로 정확히 예측했다면 가장 정확히 예측한 것이고 이탈도는 0이 됩니다. 반면, $P = 0$으로 완전 잘못 예측했다면 이탈도는 $+\infty$가 됩니다. 
 
  \ 반면, $Logistic \ Regression \ Deviance$는 $Y_{i} = 0$ 일때 다음의 수식이 됩니다. 
  
$$\sum_{i=1}^{N} \left[ -2 \ln(1-P) \right]$$
 
 \ 이 수식은 P가 0부터 1까지 변화할 때 위의 식의 값은 0에서  $+\infty$ 까지 변화합니다.
 
```{r}
myfun_3 <- function(x_var) {-2*log(1 - x_var)};
ggplot(data.frame(x= c(0, 1)), aes(x=x)) + 
  stat_function(fun = myfun_3, geom="line", size = 1.2, color = 'steelblue') + 
  labs(x = "P", y = "Logistic Reg Deviance") +
  theme_minimal() -> p_3; ggplotly(p_3)
```

  \ $Y_{i} = 0$일때 만약 결과변수가 1이 되는 확률 $P =1$ 로 완전히 잘못 예측했다면 이탈도는 $+\infty$가 됩니다. 반면 확률 $P =0$ 로 가장 정확히 예측했다면 이탈도는 0이 됩니다. 
  
 
 \ 다중회귀에서는 흔히 모든 점수의 평균을 기저 모형으로 사용합니다. 다른 어떤 정보도 없는 상태에서 결과를 예측하기 위해 사용할 수 있는 최선의 모형이 평균이기 때문입니다. 그런데 로지스틱 회귀에서는 결과변수가 0과 1로만 구성되어 있어 평균을 기저모형을 사용하기 제한됩니다. 0과 1 이외의 0.5와 같은 결과값이 애당초 존재하지 않기 때문입니다. 
 
 \ 그래서 기저모형은 0과 1중 빈도가 가장 높은 것으로 간주해 버립니다. 만약 총 사례가 100개인데 그 중 0이 49개, 1이 51개라면 기저모형은 모든 값들이 1이라고 가정한 모델이 됩니다. 즉, **다중회귀에서는 평균이 기저 모형이었지만 로지스틱 회귀에서는 가장 자주 발생한 결과가 기저모형입니다.**
 

 \ 기저모형에 더 많은 예측변수를 추가함으로써 모형을 좀 더 개선할 수 있습니다. 

$$  \chi^{2} = (-2LL(based \ model)) - (-2LL(new \ model)) $$

$$  \chi^{2} = 2LL(new \ model) - 2LL(based \ model) $$

$$ df = k_{new \ model}  - k_{based \ model}$$

 \ 카이제곱은 새 모형의 이탈도에서 기저 모형의 이탈도를 뺀 값입니다. 이 차이를 가능도비(likelihood ratio)라고 합니다. df는 자유도인데 새 모형의 매개 변수에서 기저 모형의 매개변수 개수를 뺀 것입니다. **가능도비는 자유도가 $df$인 카이제곱 분포를 따릅니다.**

\ R에서 로지스틱 회귀를 수행하면 결과값에 Null deviance와 Residual deviance가 출력됩니다. 이 때 Null deviance는 모형을 적합하기 전에 deviance이고 Residual deviance는 모형을 적합한 후의 deviance입니다. 이 둘 사이가 충분히 줄었다면 모형이 적합하다도 보게됩니다. deviance의 차이가 카이제곱 분포를 따르기에 통계적으로 유의한 차이인지 여부를 알 수 있습니다. 

\ 만약 예를 들어, deviance의 차이가 11.2가 나왔고 이 때 자유도가 1이었다고 한다면 이것의 p-value는 다음과 같은 코드로 구합니다.

```{r}
1 - pchisq(11.2, 1)
```

\ 이 값이  < .05이므로 통계적으로 유의합니다. 이럴 경우 모델이 적합되었다고 판단하게 됩니다.