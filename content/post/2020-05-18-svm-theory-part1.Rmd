---
title: SVM theory Part1
author: Daniel Kim
date: '2020-05-18'
slug: svm-theory-part1
categories: []
tags:
  - ML
  - SVM
  - R
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", message=F, warning=F, cache = F)
```


```{r message = FALSE, warning = FALSE}
library(e1071)
library(tidyverse)
library(plotly)
```

 Logistic Regression에서는 확률을 이용합니다. 바로 확률을 이용해서 Decision Boundary를 결정합니다. 그런데 Support Vector Machine은 확률을 이용하지 않습니다. 다른 방법으로 Decision Boudary를 설정합니다.
 
```{r}
# create linear_plot_with_theta_delta() funcition
# theata means a slope and delta means a margin

# set seed
set.seed(2020)
n <- 50  
df <- data.frame(x1 = runif(n), x2 = runif(n))
  

linear_plot_with_theta_delta <- function(theta, delta) {
  

  # Generate data frame with two uniformly distributed predictors lying between 0 and 1.
  # classify data points depending on location
  df$y <- factor(ifelse(df$x2 - theta*df$x1 < 0, 0, 1), 
                 levels = c(0, 1))
  
  # retain only those points that lie outside the margin
  df_1 <- df[abs(theta*df$x1 - df$x2) > delta, ]
  
  # build plot
  plot_theta_margins <- ggplot(data = df_1, aes(x = x1, y = x2, color = y)) + geom_point() + 
    scale_color_manual(values = c("red", "blue")) + 
    geom_abline(slope = theta, intercept = 0, color = "green", size = 1.2)+
    geom_abline(slope = theta, intercept = delta, linetype = "dashed") +
    geom_abline(slope = theta, intercept = -delta, linetype = "dashed") +
    theme(legend.position = "none")
    # ggtitle(paste0("slope=", as.character(theta), " delta=", as.character(delta)))
  
  #' display plot
  plot_theta_margins
  
}

linear_plot_with_theta_delta(1, .1) +
  theme_minimal()
```
 
 
```{r}
linear_plot_with_theta_delta(1, .1) +
  geom_abline(intercept = -.1, slope = 1.2, color = 'blue', size = 1.2) +
  geom_abline(intercept = .1, slope = .8, color = 'red', size = 1.2) +
  theme_minimal()
```
 
 위의 그림에서 파란 점은 목적변수가 Positive 즉 1인 경우이고 빨간 점은 반대로 목적변수가 Negative 즉 0인 겨우입니다. 2차원에서 데이터를 정확히 2개의 그룹으로 분류하는 선 즉 Decision Boudary들은 여러 경우가 있을 수 있습니다. 그 중 저는 3가지 경우, 즉 파란선, 빨간선 드리고 초록선을 그려보았습니다. 이 셋 모델 모두 데이터를 정확히 Positive와 Negative로 100퍼센트 분리합니다.  
 
 그련데 우리의 직관은 뭔가 자꾸만 초록색 선이 다른 두 개의 선보다 더 훌륭한 Decision Boudary라고 하는 것 같습니다. 왠지 그렇지 않나요? 먼저 빨간색 Decision Boundary만 놓고 생각해 볼까요?
 
```{r}
df$y <- factor(ifelse(df$x2 - 1*df$x1 < 0, 0, 1), 
                 levels = c(0, 1))
df_1 <- df[abs(1*df$x1 - df$x2) > .1, ]

df_1 %>%
  filter(y == 0) %>%
  filter(x1 > .75) %>%
  filter(x2 > .625 & x2 < .875) -> df_tmp_1

df_1 %>%
  filter(y == 1) %>%
  filter(x1 < .25) %>%
  filter(x2 > .1 & x2 < .375) -> df_tmp_2


df_tmp_1 %>%
  bind_rows(df_tmp_2) -> df_2

linear_plot_with_theta_delta(1, .1) +
  geom_abline(intercept = .1, slope = .8, color = 'red', size = 1.2) +
  geom_point(data = df_2, aes(x1, x2), size = 7, alpha = .5, color = 'purple') +
  theme_minimal() -> p; p
```
 
 
빨간선의 경우 그림에서처럼 4개의 보라색 원으로 감싸여진 데이터가 매우 Decision Boundary와 가까워집니다. 데이터라는 것은 언제나 특정 시점의 관측자에 의해 측정되어진 것이기에 그 값은 측정 시점과 관측자의 상태에 따라 변동이 생깁니다. 만약 위의 보라색 원 안에 있는 데이터들이 상하좌우로 이동을 하게 된다면 어떨까요? 어떤 경우는 Decision Boundary를 넘어가 Positive를 Negative로 혹은 Negative를 Positive로 잘못 분류하는 경우가 발생합니다. 즉, 빨간선을 Decision Boundary로 가지는 분류모델은 초록선을 Decision Boundary로 가지는 분류모델에 비해 불안한 나쁜 모델이라고 할 수 있습니다. 즉 우리의 직관이 맞는 겁니다.

그런데 어떻게 이 초록색 Decision Boundary를 잘 찾을 수 있을까요? 어떻게 이 Decision Boundary를 계산할 수 있을까요?

여기서 잠깐 한 Decision Boundary와  데이터들사이의 거리를 계산하는 문제를 생각해보겠습니다. 데이터에서 Decision Boudary로 수선의 발을 내려 그 거리를 재는 것입니다. 

```{r, fig.width = 7, fig.height = 7}
# the line's slope and intercept information
slope <- 1
intercept <- 0

perp.segment.coord <- function(x1, x2, intercept, slope){
  # finds endpoint for a perpendicular segment from the point (x1,x2) to the line
  # defined by ortho as y = a*x + b
  a <- slope  # slope
  b <- intercept  # intercept
  xp1 <- (x1 + a*x2 - a*b)/(1 + a^2)
  xp2 <- b + a*xp1
  list(x1=x1, x2=x2, xp1=xp1, xp2=xp2)
}

perp.segment <- perp.segment.coord(df_1$x1, df_1$x2, intercept, slope)
perp.segment_1 <- perp.segment %>% as.data.frame()
perp.segment_1 %>%
  mutate(y = df_1$y) -> perp.segment_2

ggplot(data = df_1, aes(x1, x2, color = y)) +
  geom_point() +
  scale_color_manual(values = c("red", "blue")) + 
  xlim(0, 1) +
  ylim(0, 1) +
  geom_abline(intercept = intercept, slope = slope) +
  geom_segment(data = perp.segment_2,
               aes(x = x1, y= x2, xend = xp1, yend = xp2), 
               lty = 'dashed') +
  theme_minimal() -> p; p
```

y가 0이고 가장 가까운 거리를 가지는 점을 찾아보겠습니다. 

```{r, fig.width = 7, fig.height = 7}
perp.segment_2 %>%
  mutate(x_diff = xp1 - x1) %>%
  mutate(y_diff = xp2 - x2) %>%
  mutate(dist_sq = x_diff^2 + y_diff^2) %>%
  mutate(dist_ucl = sqrt(dist_sq)) %>%
  arrange(dist_sq) -> df_with_dist

df_with_dist %>%
  filter(y == 0) %>%
  arrange(dist_sq) %>%
  slice(1) %>%
  select(x1, x2) -> df_tmp

p + geom_point(data= df_tmp, 
               aes(x1, x2), size = 7, alpha = .5, color = 'purple')

```

바로 이 점을 지나고 Decisions Boudary와 평행한 직선을 그어보겠습니다. 이는 기울기가 1이고 

```{r}
df_tmp
```

`x1 = 0.3942258`, `x2 = 0.2882747` 인 점을 지나는 직선을 찾는 문제가 됩니다.

```
0.2882747 = 0.3942258 * 1 + intercept
```
위의 식에서 intercept를 구하면 됩니다.

```{r}
0.2882747 - 0.3942258
```

즉, `intercept`는 `-0.1059511` 입니다. 시각화해보겠습니다.

```{r, fig.width = 7, fig.height = 7}
 p_1 <- p + 
  geom_point(data= df_tmp, aes(x1, x2), size = 7, alpha = .5, color = 'purple') + 
  geom_abline(intercept = -0.1059511, slope = 1, color = 'purple', size = 1, linetype = 'dashed'); p_1
```


```{r, fig.width = 7, fig.height = 7}
df_with_dist %>%
  filter(y == 1) %>%
  arrange(dist_sq) %>%
  slice(1) %>%
  select(x1, x2) -> df_tmp

intercept <- 0.6510358 - 0.5467153

p_1 + geom_point(data= df_tmp, 
               aes(x1, x2), size = 7, alpha = .5, color = 'purple') +
   geom_abline(intercept = intercept, slope = 1, color = 'purple', size = 1, linetype = 'dashed')
```

